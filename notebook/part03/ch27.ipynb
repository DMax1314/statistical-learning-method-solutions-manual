{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第27章预训练语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 习题27.1\n",
    "&emsp;&emsp;设计基于双向LSTM的预训练语言模型，假设下游任务是文本分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答思路：**\n",
    "\n",
    "1. 结合习题25.4可给出双向LSTM模型\n",
    "2. 给出下游任务是文本分类的模型微调的描述\n",
    "3. 自编程实现双向LSTM的预训练语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第1步：结合习题25.4可给出双向LSTM模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;前向的LSTM的隐层（状态）是\n",
    "$$\n",
    "h_t^f = \\text{LSTM}_f (x_t, h_{t-1}^f)\n",
    "$$\n",
    "后向的LSTM的隐层（状态）是\n",
    "$$\n",
    "h_t^b = \\text{LSTM}_b (x_t, h_{t+1}^b)\n",
    "$$\n",
    "其中，$\\text{LSTM}_f$ 和 $\\text{LSTM}_b$ 分别表示前向LSTM和后向LSTM，$h_{t-1}^f$ 和 $h_{t+1}^b$ 分别表示前向LSTM和后向LSTM的上一个位置的隐藏状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两者的拼接是\n",
    "$$\n",
    "h_t = [h_i^f; h_i^b]\n",
    "$$\n",
    "其中，；表示两个向量的拼接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_t = \\text{softmax}(V \\cdot h_t + c) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第2步：给出下游任务是文本分类的模型微调的描述**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> &emsp;&emsp;假设下游任务是文本分类，输入单词序列是$x' = x_0, x_1, \\cdots, x_l$，输出是类别 $y$，计算条件概率 $P(y|x_0,x_1,\\cdots, x_l)$：\n",
    "> $$\n",
    "P_{\\theta,\\phi} = \\text{softmax} ({W_y}^T h_{cls}^{(L)}) = \\frac{\\exp {w_y^T \\cdot h_{cls}^{(L)}}}{\\sum_{y'} \\exp {w_{y'}^T \\cdot h_{cls}^{(L)}}} \\tag{27.30}\n",
    "$$\n",
    ">其中，$h_{cls}^{(L)}$ 是第 $L$ 层的类别特殊字符\\<cls\\>的表示向量，$W_y$ 是类别的权重矩阵，$\\phi$ 表示分类的参数。这时单词序列 $x_0, x_1, \\cdots ,x_l$ 是一个句子或一段文章，以特殊字符\\<cls\\>开始，以特殊字符\\<sep\\>结束。\n",
    "> 微调损失函数为：\n",
    "> $$\n",
    "L_{\\text{FT}} = -\\log P_{\\theta,\\phi}(y|x')\n",
    "$$\n",
    "> 微调中，预训练模型的参数 $\\theta$ 作为初始值，在这个过程中进一步得到学习，以帮助更好地分类；同时分类的参数 $\\phi$ 也得到学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第3步：自编程实现双向LSTM的预训练语言模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到词向量\n",
    "train_iter = AG_NEWS(split='train', root='./data')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x : vocab(tokenizer(x))\n",
    "label_pipeline = lambda x : int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载AG_NEWS数据集\n",
    "train_iter, test_iter = AG_NEWS(root='./data')\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
    "        # 使用双向LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        # 使用线性函数进行文本分类任务\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x, _ = self.lstm(embedded)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "NUM_CLASSES = 4\n",
    "LEARNING_RATE = 1e-2\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(len(vocab), EMBED_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# 设置优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  5.84s | valid accuracy     92.6% \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  6.19s | valid accuracy     92.6% \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  6.21s | valid accuracy     92.4% \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  6.42s | valid accuracy     92.1% \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  6.45s | valid accuracy     91.6% \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  6.42s | valid accuracy     91.5% \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  6.12s | valid accuracy     91.8% \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  6.44s | valid accuracy     92.0% \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  6.71s | valid accuracy     91.5% \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  6.31s | valid accuracy     91.6% \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.1f}% '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val * 100))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ex_text_str = \"\"\"\n",
    "Our younger Fox Cubs (Y2-Y4) also had a great second experience \n",
    "of swimming competition in February when they travelled over to \n",
    "NIS at the end of February to compete in the SSL Development \n",
    "Series R2 event. For students aged 9 and under these SSL \n",
    "Development Series events are a great introduction to \n",
    "competitive swimming, focussed on fun and participation whilst \n",
    "also building basic skills and confidence as students build up \n",
    "to joining the full SSL team in Year 5 and beyond.\n",
    "\"\"\"\n",
    "model = model.to(\"cpu\")\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题27.2\n",
    "&emsp;&emsp;假设GPT微调的下游任务是两个文本的匹配，写出学习的目标函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答思路：**  \n",
    "\n",
    "1. 给出GPT模型的描述\n",
    "2. 给出文本匹配任务的描述\n",
    "3. 写出GPT微调的下游任务是两个文本匹配时的学习的目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第1步：给出GPT模型的描述**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第501页GPT模型的描述：\n",
    "> &emsp;&emsp;GPT模型的输入是单词序列，可以是一个句子或一段文章。首先经过输入层，产生初始的单词表示向量的序列。之后经过 $L$ 个Transformer解码层，得到单词表示向量的序列，GPT模型的输出是在单词序列各个位置上的条件概率。  \n",
    "> &emsp;&emsp;GPT预训练时，通过极大似然估计学习模型的参数。\n",
    "> $$\n",
    "L_{\\text{train}} = - \\sum_{i = 1}^n \\log P_{\\theta}(x_i | x_1, x_2, \\cdots, x_{i - 1})\n",
    "$$\n",
    "> &emsp;&emsp;GPT微调时，通过优化下游任务的目标函数，进一步调节模型的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第2步：给出文本匹配任务的描述**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第499页文本匹配任务的描述：\n",
    "> &emsp;&emsp;如果下游任务是文本匹配，如判断两句话是否形成一问一答。输入单词序列是 $x_0, x_1, \\cdots, x_l$，输出是类别 $y$，仍然计算条件概率 $P(y | x_0, x_1, \\cdots, x_l)$。类别有两类，表示匹配或不匹配。这时单词序列 $x_0, x_1, \\cdots, x_l$ 是两个单词序列合成的序列，如一个问句和一个答句合并而成。以特殊字符\\<cls\\>开始，中间以特殊字符\\<sep\\>间隔，最后以特殊字符\\<sep\\>结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第3步：写出GPT微调的下游任务是两个文本匹配时的学习的目标函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;假设分别有两个文本，矩阵$X_1 = (x_1^{(1)}, x_2^{(1)}, \\cdots, x_{m - 1}^{(1)})$和矩阵$X_2 = (x_{m + 1}^{(2)}, x_{m + 2}^{(2)}, \\cdots, x_{m + n - 1}^{(2)})$分别表示两个文本的词嵌入的序列，矩阵$E_1=(e_1^{(1)}, e_2^{(1)}, \\cdots, e_{m - 1}^{(1)})$和矩阵$E_2=(e_{m + 1}^{(2)}, e_{m + 2}^{(2)}, \\cdots, e_{m + n - 1}^{(2)})$分别表示两个文本的位置嵌入序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;经过输入层，产生初始的单词表示向量的序列，分别表示为\n",
    "$$\n",
    "H_1^{0} = X_1 + E_1 \\\\\n",
    "H_2^{0} = X_2 + E_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;将两个句子分别作为前句或后句，构造两个完整的文本：  \n",
    "- 文本1：\\<cls\\>, $x_1^{(1)}, x_2^{(1)}, \\cdots, x_{m - 1}^{(1)},$ \\<sep\\>, $x_{m + 1}^{(2)}, x_{m + 2}^{(2)}, \\cdots, x_{m + n - 1}^{(2)}$, \\<sep\\>\n",
    "- 文本2：\\<cls\\>, $x_{m + 1}^{(2)}, x_{m + 2}^{(2)}, \\cdots, x_{m + n - 1}^{(2)},$ \\<sep\\>, $x_1^{(1)}, x_2^{(1)}, \\cdots, x_{m - 1}^{(1)}$, \\<sep\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别进行GPT预训练模型：\n",
    "$$\n",
    "H_1^{(L)} = \\text{transformer_decoder}([H_1^{(0)};H_2^{(0)}]) \\\\\n",
    "H_2^{(L)} = \\text{transformer_decoder}([H_2^{(0)};H_1^{(0)}]) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将得到单词表示向量的序列进行融合，使用线性层得到GPT模型的输出：\n",
    "$$\n",
    "H'^{(L)} = H_1^{(L)} \\oplus H_2^{(L)} = (h_1'^{(L)}, h_2'^{(L)}, \\cdots h_{m + n}'^{(L)}) \\\\\n",
    "P_{\\theta}(x_i |x_1, x_2, \\cdots, x_{i - 1}) = \\text{softmax} (W_x^T h_i'^{(L)}) = \\frac{\\exp \\left ( w_{x_i}^T \\cdot h_i'^{(L)} \\right )}{\\displaystyle \\sum_{x_i} \\exp \\left (w_{x_i}^T \\cdot h_i'^{(L)} \\right )}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在预训练阶段，目标函数是\n",
    "$$\n",
    "L_{\\text{PT}} = - \\sum_{i=1}^{m + n} \\log P_{\\theta}(x_i |x_1, x_2, \\cdots, x_{i - 1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;微调时，下游任务是文本匹配，输入的单词序列是两个文本合的序列$x_1, x_2, \\cdots,x_{m + n}$，输出是类别$y$，以特殊字符\\<cls\\>开始，中间以特殊字符\\<sep\\>间隔，最后以特殊字符\\<sep\\>结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可得到微调的目标函数为\n",
    "$$\n",
    "L_{\\text{LM}} = - \\frac{1}{m + n} \\sum_{i = 1}^{m + n} \\log P_{\\theta}(y|x_1, x_2, \\cdots, x_{i - 1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题27.3\n",
    "&emsp;&emsp;设计一个2层卷积神经网络编码器和2层卷积神经网络解码器组成的自动编码器（使用第28章介绍的转置卷积）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答思路：**  \n",
    "\n",
    "1. 给出自动编码器的原理\n",
    "2. 自编程实现2层卷积自动编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第1步：自动编码器的原理**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第493页关于自动编码器的描述：\n",
    "> &emsp;&emsp;自动编码器（auto encoder）是用于数据表示的无监督学习的一种神经网络。自动编码器由编码器网络和解码器网络组成。\n",
    "> 最基本的情况下，编码器和解码器分别都是一层神经网络\n",
    "> $$\n",
    "z = F(x) = a(W_E x + b_E) \\\\\n",
    "y = G(z) = a(W_D x + b_D)\n",
    "$$\n",
    "> 其中，$W_E, W_D$是权重矩阵，$b_E, b_D$是偏置向量，$a(\\cdot)$是激活函数。  \n",
    "> &emsp;&emsp;学习时，目标函数是\n",
    "> $$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^N L(x_i, y_i) = \\frac{1}{N} \\sum_{i = 1}^N L(x_i, G(F(x_i)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第2步：自编程实现2层卷积自动编码器**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import mnist\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # 2层卷积神经网络编码器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, 2, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 2层卷积神经网络解码器\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 使用转置卷积\n",
    "            nn.ConvTranspose2d(32, 16, 3, 2, 1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(16, 1, 3, 2, 1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 使用MNIST数据集\n",
    "train_set = mnist.MNIST('./data', transform=transforms.ToTensor(), train=True, download=True)\n",
    "test_set = mnist.MNIST('./data', transform=transforms.ToTensor(), train=False, download=True)\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder().to(device)\n",
    "\n",
    "# 设置损失函数\n",
    "criterion = nn.MSELoss()\n",
    "# 设置优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 显示模型结构\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "EPOCHES = 10\n",
    "for epoch in range(EPOCHES):\n",
    "    for img, _ in tqdm.tqdm(train_dataloader, ):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img = img.to(device)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, img)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 将生成图片和原始图片进行对比\n",
    "for i, data in enumerate(test_dataloader):\n",
    "    img, _ = data\n",
    "    img = img.to(device)\n",
    "    model = model.to(device)\n",
    "    img_new = model(img).detach().cpu().numpy()\n",
    "    img = img.cpu().numpy()\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    for j in range(8):\n",
    "        plt.subplot(2, 8, j + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img_new[j].squeeze())\n",
    "        plt.subplot(2, 8, 8 + j + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img[j].squeeze())\n",
    "    if i >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_structure(model, device):\n",
    "    x = torch.randn(1, 1, 28, 28).requires_grad_(True).to(device)\n",
    "    y = model(x)\n",
    "    vise = make_dot(y, params=dict(list(model.named_parameters()) + [('x', x)]))\n",
    "    vise.format = \"png\"\n",
    "    vise.directory = \"./data\"\n",
    "    vise.view(\"2-Layer-CNN-AutoEncoder-Struction\", cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型架构图\n",
    "save_model_structure(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题27.4\n",
    "\n",
    "&emsp;&emsp;证明当编码器和解码器都是线性函数时，主成分分析可以作为自动编码器学习的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答思路：**  \n",
    "\n",
    "1. 给出主成分分析的定义\n",
    "2. 给出自动编码器学习过程\n",
    "3. 证明当编码器和解码器都是线性函数时，主成分分析可以作为自动编码器学习的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第1步：主成分分析的定义**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据书中第250页主成分分析的基本思想\n",
    "> &emsp;&emsp;主成分分析中，首先对给定数据进行规范化，使得数据每一变量的平均值为0，方差为1，之后对数据进行正交变换，原来由线性相关变量表示的数据通过正交变换变成由若干个线性无关的新变量表示的数据。新变量是可能的正交变换中变量的方差的和（信息保存）最大的，方差表示在新变量上信息的大小。将新变量依次称为第一主成分、第二主成分等。这就是主成分分析的基本思想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据书中第266页算法16.1主成分分析算法\n",
    "> **算法16.1（主成分分析算法）**  \n",
    "> 输入：$m \\times n$样本矩阵$X$，其每一行元素的均值为零；  \n",
    "> 输出：$k \\times n$样本主成分矩阵$Y$。  \n",
    "> 参数：主成分个数$k$  \n",
    "> （1）构造新的$n \\times m$矩阵\n",
    "> $$\n",
    "X' = \\frac{1}{\\sqrt{n - 1}} X^T\n",
    "$$\n",
    "> $X'$每一列的均值为零。  \n",
    "> （2）对矩阵$X'$进行截断奇异值分解，得到\n",
    "> $$\n",
    "X' = U \\Sigma V^T\n",
    "$$\n",
    "> 有$k$个奇异值、奇异向量。矩阵$V$的前$k$列构成$k$个样本主成分。  \n",
    "> （3）求$k \\times n$样本主成分矩阵\n",
    "> $$\n",
    "Y = V^T X\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第2步：自动编码器学习过程**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第501页关于自动编码器的描述：\n",
    "> &emsp;&emsp;自动编码器是用于数据表示的无监督学习的一种神经网络。自动编码器由编码器网络和解码器网络组成。学习时编码器将输入向量转换为中间表示向量，解码器再将中间表示向量转换为输出向量。编码器和解码器可以是\n",
    "> $$\n",
    "z = F(x) = a(W_E x + b_E) \\\\\n",
    "y = G(z) = a(W_D z + b_D)\n",
    "$$\n",
    "> 学习的目标是尽量使输出向量和输入向量保持一致，或者说重建输入向量。认为学到的中间表示向量就是数据的表示。\n",
    "> $$\n",
    "L = \\frac{1}{N} \\sum_{i = 1}^N L(x_i, G(F(x_i)))\n",
    "$$\n",
    "> 学习的算法一般是梯度下降。自动编码器学习实际进行的是对数据的压缩。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第3步：证明当编码器和解码器都是线性函数时，主成分分析可以作为自动编码器学习的方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据题意，当编码器和解码器都是线性函数时，即\n",
    "$$\n",
    "F(x) = W_E x \\\\\n",
    "G(z) = W_D z\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习时，则自动编码器的目标函数为\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i = 1}^N L(x_i, W_D W_E x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: bold\"><font color=\"red\">待补充</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题27.5\n",
    "\n",
    "&emsp;&emsp;解释为什么BERT预训练中的掩码语言模型化是基于去噪自动编码器原理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答思路：**  \n",
    "\n",
    "1. 给出BERT预训练中的掩码语言模型化描述\n",
    "2. 给出去噪自动编码器的原理\n",
    "3. 解释BERT预训练中的掩码语言模型化是基于去噪自动编码器原理的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第1步：BERT预训练中的掩码语言模型化的描述**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第497页关于BERT预训练中的掩码语言模型化的描述：\n",
    "> &emsp;&emsp;BERT模型的预训练由两部分组成，掩码语言模型化和下句预测。掩码语言模型化的目标是复原输入单词序列中被掩码的单词。可以看作是去噪自动编码器学习，对被掩码的单词独立地进行复原。掩码单词序列表示为$\\tilde{x}$。  \n",
    "> &emsp;&emsp;掩码语言模型化在每一个掩码位置计算条件概率：\n",
    "> $$\n",
    "P_{\\theta} (x_i | \\tilde{x}_0, \\tilde{x}_1, \\cdot, \\tilde{x}_{m + n}) = \\text{softmax} \\left( W_x^T h_i^{(L)} \\right) = \\frac{\\exp \\left( w_{x_i}^T \\cdot h_i^{(L)} \\right)}{\\displaystyle \\sum_{x'_i} \\exp \\left( w_{x_i}^T \\cdot h_i^{(L)} \\right)}\n",
    "$$\n",
    "> 假设第$i$个位置是掩码位置，$h_i^{(L)}$是在第$L$层第$i$个位置的表示，$x_i$是预测的单词，$W_{x}$是单词的权重矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第2步：去噪自动编码器的原理**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第501页去噪自动编码器的描述：\n",
    "> &emsp;&emsp;去噪自动编码器是自动编码器的一种扩展，去噪自动编码器不仅可以用于数据表示学习，而且可以用户数据去噪。学习时首先根据对输入向量进行的随机变化，得到有噪声的输入向量。编码器将有噪声的输入向量转换为中间表示向量，解码器再将中间表示向量转换为输出向量。编码器、解码器、目标函数分别是\n",
    "> $$\n",
    "z = F(x) = a(W_E \\tilde{x} + b_E) \\\\\n",
    "y = G(z) = a(W_D z + b_D) \\\\\n",
    "L = \\frac{1}{N} \\sum_{i = 1}^N L(x_i, G(F(\\tilde(x)_i)))\n",
    "$$\n",
    "> 学习的目标是尽量使输出向量和原始输入向量保持一致，或者说重建原始输入向量。因为学习的目标是排除噪声的干扰重建数据，去噪自动编码器能更有效地学到数据的主要特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第3步：解释BERT预训练中的掩码语言模型化是基于去噪自动编码器原理的原因**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;BERT在预训练阶段，输入序列中的一些单词被随机遮掩，得到有噪声的输入向量，模型需要预测这些单词。这个过程类似于去噪自动编码器的学习过程，其中模型需要从缺失的或噪声的数据中重构原始数据。因此，掩码语言模型化提供了一些“噪音”，让模型学会从不完整的输入中推断出缺失的信息，从而提高模型的泛化能力。这也就说明了BERT预训练中的掩码语言模型化是基于去噪自动编码器原理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题27.6\n",
    "\n",
    "&emsp;&emsp;比较BERT与Transformer编码器在模型上的异同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答思路：**  \n",
    "\n",
    "1. 给出BERT的模型描述\n",
    "2. 给出Transformer编码器的模型描述\n",
    "3. 比较BERT与Transformer编码器在模型上的异同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第1步：BERT模型的描述**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第495页关于BERT的描述：\n",
    "> &emsp;&emsp;BERT是双向Transformer编码器表示（Bidirectional Encoder Representations from Transformers）的缩写。BERT的模型基于Transformer的编码器，是双向语言模型。BERT的预训练主要是掩码语言模型化，使用大规模语料基于自去噪自动编码器原理进行模型的参数估计，学习的目标是复原给定的掩码单词序列中被掩码的每一个单词。学习和预测都是非自回归过程（Non-autoregressive process）。\n",
    "> &emsp;&emsp;每一个单词序列是一个句子或一段文章。首先经过输入层，产生初始的单词表示向量的序列，记作矩阵$H^{(0)}$：\n",
    "> $$\n",
    "H^{(0)} = X + S + E \\tag{27.22}\n",
    "$$\n",
    "> 其中，矩阵$X$表示单词的词嵌入的序列$X = (x_0, x_1, \\cdots, x_{m + n})$；矩阵$E$表示单词的位置嵌入的序列$E=(e_0, e_1, \\cdots, e_{m + n})$；矩阵$S$是区别前后单词的标记序列$S=(a, a, \\cdots, a, b, b, \\cdots, b)$，含有$m + 1$个向量$a$和$n$个向量$b$。$X,E,S,H^{(0)}$是$d \\times (m + n + 1)$矩阵，设词嵌入、位置嵌入、标记向量的维度是$d$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第502页BERT模型的描述：\n",
    "> &emsp;&emsp;BERT模型的输入是两个合并的单词序列。首先经过输入层，产生初始的单词表示向量的序列。之后经过$L$个Transformer编码层，得到单词的表示向量的序列。BERT模型的输出是在单词序列的各个位置上的条件概率。  \n",
    "> &emsp;&emsp;BERT模型的预训练由掩码语言模型化和下句预测组成。掩码语言模型化的目标是复原输入单词序列中被掩码的单词。下句预测的目标是判断输入单词序列是否来自同一篇文章。预训练以掩码语言模型化为主，其损失函数是\n",
    "> $$\n",
    "L_1 = - \\sum_{i = 0}^{m + n} \\delta_i \\log P_{\\theta} (x_i | \\tilde{x}_0, \\tilde{x}_1, \\cdots, \\tilde{x}_{m + n})\n",
    "$$\n",
    "> &emsp;&emsp;BERT微调时，通过优化下游任务的目标函数，进一步调节模型的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第2步：Transformer编码器的模型描述**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第476页关于Transformer编码器的描述：  \n",
    "> &emsp;&emsp;Transformer（转换器）由编码器和解码器组成。编码器有1个输入层、6个编码层（一般是$L$层）。解码器有1个输入层、6个解码层、1个输出层。编码器的输入层与第1个编码层连接，第1个编码层再与第2个编码层连接，依次连接，直到第6个编码层。解码器的输入层与第1个解码层连接，第1个解码层再与第2个解码层连接，依次连接，直到第6个解码层。第6个解码层再与输出层连接。第6个编码层与各个解码层之间也有连接。  \n",
    "> &emsp;&emsp;编码器的6个编码层将输入单词序列进行转换，得到中间表示序列。  \n",
    "> &emsp;&emsp;编码器的6个编码层有相同的结构，每一个编码层由自注意力子层和前馈网络子层两部分组成。  \n",
    "> &emsp;&emsp;在编码器的输入层，输入序列的各个位置有单词的词嵌入（word embedding）和位置嵌入（position embedding），其中位置嵌入表示在序列中的位置。在每一个位置以词嵌入和位置嵌入的和作为该位置的输入向量。单词的词嵌入通常通过对单词的独热向量进行一个线性变化得到，即用一个矩阵乘以独热向量，矩阵称为词嵌入矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据书中第481页关于Transformer编码器计算公式：\n",
    "> &emsp;&emsp;在编码器的输入层通过线性变换获得单词的词嵌入。\n",
    "> $$\n",
    "e = W_e \\cdot w \\tag{26.26}\n",
    "$$\n",
    "> 其中，$w$是单词的独热向量，$e$是单词的词嵌入，$W_e$是嵌入矩阵。嵌入矩阵在学习中自动获得。  \n",
    "> &emsp;&emsp;编码器的输入层的每一个位置的输入向量是\n",
    "$$\n",
    "e + p \\tag{26.27}\n",
    "$$\n",
    "其中，$e$是该位置的词嵌入，$p$是该位置的位置嵌入。  \n",
    "> &emsp;&emsp;编码器的每一层的每一个位置的前馈网络是\n",
    "> $$\n",
    "\\text{ffn} (z) = W_2 \\text{relu} (W_1 z + b_1) + b_2 \\tag{26.28}\n",
    "$$\n",
    "其中，$W_1$和$W_2$是权重矩阵，$b_1$和$b_2$是偏置向量。  \n",
    "> &emsp;&emsp;编码器的每一层的每一个位置的层归一化函数是\n",
    "> $$\n",
    "\\text{norm} (z) = \\gamma \\frac{z - u \\cdot \\boldsymbol{1}}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta \\cdot \\boldsymbol{1}\n",
    "$$\n",
    "> 其中，$u$是均值，$\\sigma^2$是方差，$\\gamma$和$\\beta$是参数，$\\varepsilon$是常量。  \n",
    "> &emsp;&emsp;编码器的输入层的计算可以写作\n",
    "> $$\n",
    "H_E^{(0)} = E_E + P_E \\tag{26.31}\n",
    "$$\n",
    "> &emsp;&emsp;编码器的第$l$个编码器的多头自注意力子层和前馈网络子层计算可以写作\n",
    "> $$\n",
    "Z_E^{(l)} = \\text{norm} (H_E^{(l - 1)} + \\text{multi_attend} (H_E^{(l - 1)}, H_E^{(l - 1)}, H_E^{(l - 1)})) \\tag{26.32}\n",
    "$$\n",
    "> $$\n",
    "H_E^{(l)} = \\text{norm} (Z_E^{(l)} + \\text{ffn} (Z_E^{(l)})) \\tag{26.33}\n",
    "$$\n",
    "> 其中，$H_E^{(l)}$是第$l$个编码层的所有位置的输出，$H_E^{(l - 1)}$是所有位置的输入，$Z_E^{(l)}$是中间结果；$\\text{ffn}()$和$\\text{norm}()$的计算针对矩阵的每一列，$\\text{multi_attend()}$的计算针对矩阵整体进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第3步：比较BERT与Transformer编码器在模型上的异同**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 相同点：\n",
    "    1. 两者都由多个Transformer块组成，每个块包含多头自注意力，前馈网络和层归一化。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不同点：\n",
    "    1. BERT模型在Transformer模型的基础上添加了掩码语言模型和下句预测任务。\n",
    "    2. BERT模型在训练的时候会在其后添加线性层和softmax层，用于预测掩码语言模型和下句预测任务；Transformer编码器则是直接和Transformer解码器连接，用于预测序列的下一个单词。\n",
    "    3. BERT模型中输入序列的嵌入由词嵌入、位置嵌入、区别前后单词的标记序列的和组成；Transformer编码器模型的输入序列的嵌入由词嵌入、位置嵌入的和组成。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c50c7698ff25eb1d7dce4c52e3d5cc14b3b23d6a97d4fb70aad8c815eba6f63e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
